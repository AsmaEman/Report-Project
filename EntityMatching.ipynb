{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Hsve_Y5u2Uv",
        "outputId": "50dd98cc-f400-42a7-828b-a9a6c428a0cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.10.3-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.0/49.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Collecting pdfminer.six==20221105 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (9.4.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.24.0-py3-none-manylinux_2_17_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->pdfplumber) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->pdfplumber) (41.0.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (2.21)\n",
            "Installing collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20221105 pdfplumber-0.10.3 pypdfium2-4.24.0\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHvE5UtNVLoU",
        "outputId": "dccbd911-cb3a-4eaa-bf45-66a432e4e379"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-01 17:02:38.374644: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-01 17:02:38.374721: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-01 17:02:38.374779: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-01 17:02:38.388239: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-01 17:02:39.745846: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XVZkxAHVdnY",
        "outputId": "9bc75591-739e-4e4b-fdfc-fd849bb75573"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF\n",
        "import fitz  # PyMuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4isPnbH9Vxb-",
        "outputId": "ea645c74-0b5e-4ad8-8371-734d35b36f78"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.23.7-cp310-none-manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.23.7 (from PyMuPDF)\n",
            "  Downloading PyMuPDFb-1.23.7-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n",
            "Successfully installed PyMuPDF-1.23.7 PyMuPDFb-1.23.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = ''\n",
        "    for page_num in range(doc.page_count):\n",
        "        page = doc[page_num]\n",
        "        text += page.get_text(\"text\")\n",
        "    return text\n",
        "\n",
        "pdf_path = '/content/sample_data/calcrim_2023_edition.pdf'\n",
        "pdf_text = extract_text_from_pdf(pdf_path)"
      ],
      "metadata": {
        "id": "1b6Et011V3IX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pdf_text[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORY9qqlkaAFg",
        "outputId": "9b5f2ab6-de2d-4d72-ea4d-6568ad649566"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Judicial Council of California\n",
            "Criminal Jury Instructions\n",
            "CALCRIM\n",
            "2023\n",
            "1\n",
            "Series 100–1800\n",
            "Judicial Council of California\n",
            "Advisory Committee on Criminal Jury Instructions\n",
            "Hon. Jeffrey S. Ross, Chair\n",
            "LexisNexis Matthew Bender\n",
            "Official Publisher\n",
            "This version provided by LexisNexis® Matthew Bender®, Official Publisher, 800-533-1637,\n",
            "store.lexisnexis.com, for public and internal court use\n",
            "QUESTIONS ABOUT THIS PUBLICATION?\n",
            "For questions about the Editorial Content appearing in these volumes or reprint \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    # Add more preprocessing steps as needed\n",
        "    return text\n",
        "\n",
        "preprocessed_text = preprocess_text(pdf_text)"
      ],
      "metadata": {
        "id": "6WB9XXuuaRkd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "aDad-0CQaXV9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_large_text(text):\n",
        "    # Split the text into batches (adjust the batch size as needed)\n",
        "    batch_size = 100000\n",
        "    batches = [text[i:i + batch_size] for i in range(0, len(text), batch_size)]\n",
        "\n",
        "    # Process each batch\n",
        "    results = []\n",
        "    for batch in batches:\n",
        "        doc = nlp(batch)\n",
        "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "        results.extend(entities)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "fGXxtrPDaeqM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_large_text_streaming(text):\n",
        "    # Create a Doc object with streaming processing\n",
        "    doc = nlp.make_doc(text)\n",
        "\n",
        "    # Process the Doc in chunks\n",
        "    results = []\n",
        "    for chunk in doc.noun_chunks:\n",
        "        # Process each chunk\n",
        "        entities = [(ent.text, ent.label_) for ent in nlp(chunk.text).ents]\n",
        "        results.extend(entities)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "rFdDTSrFb1fB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx\n",
        "import spacy\n",
        "from docx import Document"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYCy8lMc4Y0x",
        "outputId": "cd4f9a03-c372-4f14-b8bf-66a4db1e01d5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.0-py3-none-any.whl (239 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/239.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/239.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.5.0)\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load English NLP model\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "gFAt6p914wJW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "\n",
        "# Example paths to PDF files\n",
        "pdf1_path = '/content/sample_data/Incident_Report.pdf'\n",
        "pdf2_path = '/content/sample_data/calcrim_2023_edition.pdf'\n",
        "\n",
        "# Function to extract text from a PDF file\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    try:\n",
        "        with fitz.open(pdf_path) as pdf_document:\n",
        "            text = \"\"\n",
        "            for page_number in range(pdf_document.page_count):\n",
        "                page = pdf_document[page_number]\n",
        "                text += page.get_text()\n",
        "            return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from {pdf_path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Load the entire content of both PDF files\n",
        "pdf1_text = extract_text_from_pdf(pdf1_path)\n",
        "pdf2_text = extract_text_from_pdf(pdf2_path)\n",
        "\n",
        "# Set the maximum number of characters or lines to print\n",
        "max_characters_limit = 500  # Set your desired character limit\n",
        "max_lines_limit = 10        # Set your desired line limit\n",
        "\n",
        "# Print a limited portion of text from each PDF file\n",
        "print(\"PDF 1 Text:\")\n",
        "print(pdf1_text[:max_characters_limit])\n",
        "# or, if you want to limit by lines:\n",
        "# print(\"\\n\".join(pdf1_text.split('\\n')[:max_lines_limit]))\n",
        "\n",
        "print(\"\\nPDF 2 Text:\")\n",
        "print(pdf2_text[:max_characters_limit])\n",
        "# or, if you want to limit by lines:\n",
        "# print(\"\\n\".join(pdf2_text.split('\\n')[:max_lines_limit]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzNDCIOy4x4J",
        "outputId": "44576fb8-8784-43d5-97cb-eb446c770fcc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDF 1 Text:\n",
            "Report Number: [123] \n",
            "  Date of Report: [2023-01-15] \n",
            "  Reporting Officer: [Officer Smith, Badge Number 456] Assignment: [Investigation] \n",
            "      I. INCIDENT DETAILS   \n",
            "1.   Date and Time of Incident: [2023-01-10, 14:30] \n",
            "2.   Location of Incident: [123 Main Street, City ville] \n",
            "3.   Nature of Incident: [Robbery] \n",
            "4.   Injuries Observed: [None] \n",
            "5.   Initial Observations: [Unknown suspect entered the store] \n",
            " \n",
            "      II. VICTIM INFORMATION   \n",
            "1.   Name: [John Doe] \n",
            "2.   Address: [456 Oak Avenue, To\n",
            "\n",
            "PDF 2 Text:\n",
            "Judicial Council of California\n",
            "Criminal Jury Instructions\n",
            "CALCRIM\n",
            "2023\n",
            "1\n",
            "Series 100–1800\n",
            "Judicial Council of California\n",
            "Advisory Committee on Criminal Jury Instructions\n",
            "Hon. Jeffrey S. Ross, Chair\n",
            "LexisNexis Matthew Bender\n",
            "Official Publisher\n",
            "This version provided by LexisNexis® Matthew Bender®, Official Publisher, 800-533-1637,\n",
            "store.lexisnexis.com, for public and internal court use\n",
            "QUESTIONS ABOUT THIS PUBLICATION?\n",
            "For questions about the Editorial Content appearing in these volumes or reprint \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define specific entity categories for NER\n",
        "entity_categories = [\"DATE\", \"TIME\", \"GPE\", \"PERSON\", \"ORG\", \"CRIME\"]\n",
        "\n",
        "# Extract key entities from PDF 1 based on headings and specific categories\n",
        "criteria_entities = []\n",
        "for ent in nlp(pdf1_text).ents:\n",
        "    if ent.label_ in entity_categories:\n",
        "        criteria_entities.append((ent.text, ent.label_))"
      ],
      "metadata": {
        "id": "pVyQiL2p6gz9"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English NLP model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Set a higher max_length limit (adjust as needed)\n",
        "nlp.max_length = 2000000  # Set a value appropriate for your text length\n",
        "\n",
        "\n",
        "# Chunk size (adjust as needed)\n",
        "chunk_size = 100000\n",
        "\n",
        "\n",
        "\n",
        "# Split the text into chunks\n",
        "chunks = [pdf2_text[i:i + chunk_size] for i in range(0, len(pdf2_text), chunk_size)]\n",
        "\n",
        "# Process each chunk separately and accumulate the results\n",
        "doc2 = None\n",
        "for chunk in chunks:\n",
        "    doc_chunk = nlp(chunk)\n",
        "    if doc2 is None:\n",
        "        doc2 = doc_chunk\n",
        "    else:\n",
        "        doc2 += doc_chunk\n",
        "\n",
        "# Search logic: Find entities in PDF 2 based on criteria from PDF 1\n",
        "matched_entities = []\n",
        "\n",
        "\n",
        "for criteria_entity, criteria_label in criteria_entities:\n",
        "    for search_ent in doc2.ents:\n",
        "        # Check if the criteria entity matches a search entity\n",
        "        if criteria_entity.lower() in search_ent.text.lower():\n",
        "            matched_entities.append((criteria_entity, criteria_label, search_ent.text, search_ent.label_))\n",
        "\n",
        "# Display matched entities\n",
        "for match in matched_entities:\n",
        "    print(f\"Matched: {match[0]} ({match[1]}) in PDF 1 with {match[2]} ({match[3]}) in PDF 2\")\n"
      ],
      "metadata": {
        "id": "kAMtDjfA6xm4"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a template for the output report\n",
        "report_template = \"\"\"\n",
        "Report:\n",
        "\n",
        "Matched Entities:\n",
        "{matched_entities}\n",
        "\n",
        "Conclusions and Hypotheses:\n",
        "[Provide your conclusions and hypotheses based on the matched entities]\n",
        "\n",
        "Penal Code of Crime:\n",
        "[Provide the penal code of the crime that closely matches the search criteria]\n",
        "\n",
        "... (add more sections as needed)\n",
        "\"\"\"\n",
        "\n",
        "# Populate the report template with actual information\n",
        "final_report = report_template.format(\n",
        "    matched_entities=\"\\n\".join([f\"{match[0]} ({match[1]}) in PDF 1 with {match[2]} ({match[3]})\" for match in matched_entities]),\n",
        "    # Add more fields as needed\n",
        ")\n",
        "\n",
        "# Display or save the final report\n",
        "print(final_report)\n",
        "\n",
        "# Save the modified Word document\n",
        "output_path = '/content/drive/MyDrive/Police_Report_Project/final_report.docx'\n",
        "doc = Document()\n",
        "doc.add_paragraph(final_report)\n",
        "doc.save(output_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMOw7Pm59YIo",
        "outputId": "6a860a62-d6ae-4bdb-ccaa-21edad450762"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Report:\n",
            "\n",
            "Matched Entities:\n",
            "\n",
            "\n",
            "Conclusions and Hypotheses:\n",
            "[Provide your conclusions and hypotheses based on the matched entities]\n",
            "\n",
            "Penal Code of Crime:\n",
            "[Provide the penal code of the crime that closely matches the search criteria]\n",
            "\n",
            "... (add more sections as needed)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract all entities from PDF 1\n",
        "pdf1_entities = [(ent.text.lower(), ent.label_) for ent in nlp(pdf1_text).ents]\n",
        "\n",
        "\n",
        "# Perform NER on PDF 2 (search area)\n",
        "doc2 = nlp(pdf2_text)\n",
        "\n",
        "# Debugging: Print extracted entities from PDF 1\n",
        "print(\"Extracted Entities from PDF 1:\")\n",
        "for ent in pdf1_entities:\n",
        "    print(f\"{ent[0]} ({ent[1]})\")\n",
        "\n",
        "# Define specific entity categories for NER\n",
        "entity_categories = set(label for _, label in pdf1_entities)\n",
        "\n",
        "# Search logic: Find entities in PDF 2 based on criteria from PDF 1\n",
        "matched_entities = []\n",
        "\n",
        "for criteria_entity, criteria_label in pdf1_entities:\n",
        "    for search_ent in doc2.ents:\n",
        "        # Case-insensitive matching\n",
        "        if criteria_entity.lower() in search_ent.text.lower():\n",
        "            matched_entities.append((criteria_entity, criteria_label, search_ent.text, search_ent.label_))\n",
        "\n",
        "# Debugging: Print matched entities for verification\n",
        "print(\"Matched Entities:\")\n",
        "for match in matched_entities:\n",
        "    print(f\"Matched: {match[0]} ({match[1]}) in PDF 1 with {match[2]} ({match[3]}) in PDF 2\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIIYNrft-rBb",
        "outputId": "e7fc388c-a463-43b2-8eb4-cc1d8a457276"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Entities from PDF 1:\n",
            "123 (CARDINAL)\n",
            "2023-01-15 (DATE)\n",
            "smith (PERSON)\n",
            "456 (CARDINAL)\n",
            "i. incident (ORG)\n",
            "1 (CARDINAL)\n",
            "time of incident (ORG)\n",
            "2023-01-10 (DATE)\n",
            "2 (CARDINAL)\n",
            "location of incident (ORG)\n",
            "123 (CARDINAL)\n",
            "main street (FAC)\n",
            "3 (CARDINAL)\n",
            "4 (CARDINAL)\n",
            "injuries observed (PERSON)\n",
            "5 (CARDINAL)\n",
            "1 (CARDINAL)\n",
            "john doe] \n",
            " (PERSON)\n",
            "2 (CARDINAL)\n",
            "456 (CARDINAL)\n",
            "townsville (GPE)\n",
            "3 (CARDINAL)\n",
            "555-1234 (DATE)\n",
            "john.doe@email.com (PERSON)\n",
            "4 (CARDINAL)\n",
            "victim (PERSON)\n",
            "1 (CARDINAL)\n",
            "jane smith (PERSON)\n",
            "555 (CARDINAL)\n",
            "1 (CARDINAL)\n",
            "2 (CARDINAL)\n",
            "25-30 (CARDINAL)\n",
            "approximately 6 feet (QUANTITY)\n",
            "3 (CARDINAL)\n",
            "park street] \n",
            " \n",
            "      v. evidence (FAC)\n",
            "1 (CARDINAL)\n",
            "photographs (PERSON)\n",
            "2 (CARDINAL)\n",
            "3 (CARDINAL)\n",
            "4 (CARDINAL)\n",
            "1 (CARDINAL)\n",
            "interviews conducted (FAC)\n",
            "victim (PERSON)\n",
            "2 (CARDINAL)\n",
            "3 (CARDINAL)\n",
            "additional investigative actions (ORG)\n",
            "1 (CARDINAL)\n",
            "2 (CARDINAL)\n",
            "3 (CARDINAL)\n",
            "johnson (PERSON)\n",
            "officer's narrative of the incident (WORK_OF_ART)\n",
            "any initial conclusions (ORG)\n",
            "2023-01-15 (DATE)\n",
            "Matched Entities:\n"
          ]
        }
      ]
    }
  ]
}